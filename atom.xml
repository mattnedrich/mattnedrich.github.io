<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Matt Nedrich]]></title>
  <link href="http://mattnedrich.github.io/atom.xml" rel="self"/>
  <link href="http://mattnedrich.github.io/"/>
  <updated>2014-02-20T10:01:28-05:00</updated>
  <id>http://mattnedrich.github.io/</id>
  <author>
    <name><![CDATA[Matt Nedrich]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Quicksort in C#]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/02/17/quicksort-in-c-number/"/>
    <updated>2014-02-17T13:36:31-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/02/17/quicksort-in-c-number</id>
    <content type="html"><![CDATA[<p>I have written an implementation of quicksort in C#. It can be found here.<br />
<a href="https://github.com/mattnedrich/algorithms/blob/master/sorting/quicksort/csharp/Quicksorter.cs">https://github.com/mattnedrich/algorithms/blob/master/sorting/quicksort/csharp/Quicksorter.cs</a></p>

<p>It operates on anything that implements <code>IComparable&lt;T&gt;</code>. Suppose you have a class <code>SortableObject</code> that implements <code>IComparable&lt;SortableObject&gt;</code>. The usage model in this scenario would be</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="predefined-type">List</span>&lt;SortableObject&gt; items = ...
Quicksorter&lt;SortableObject&gt; sorter = <span class="keyword">new</span> Quicksorter&lt;SortableObject&gt;();
sorter.Sort(items);
</pre></div>
</div>
 </figure></notextile></div>

<p>My implementation contains a main <code>Partition</code> method that is called recursively to perform the sorting. In addition, there are two helper methods. The first, <code>ChoosePivotIndex</code> is responsible for choosing a pivot element from a list to be sorted. The current implementation naively selects a pivot at random. This method is virtual to keep it <a href="http://en.wikipedia.org/wiki/Open/closed_principle">Open/Closed</a> and allow others to extend it with a better pivot selection method if desired. The other method, <code>Swap</code>, swaps two elements in a list. I&rsquo;ve found that in-lining the swapping yields faster performance, though I decided not to do so for clarity.</p>

<p>I benchmarked my implementation on different types of input of varying length. Results below.</p>

<p><img width="425px" src="http://mattnedrich.github.io/images/quicksort_csharp/quicksort_perf.png" /></p>

<p>As displayed, my implementation (even with the naive pivot selection) is fairly robust to random, sorted, and reverse sorted lists.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[C# Sorting Woes]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/02/16/c-sharp-sorting-woes/"/>
    <updated>2014-02-16T16:22:30-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/02/16/c-sharp-sorting-woes</id>
    <content type="html"><![CDATA[<p>I was having some fun implementing sorting algorithms and thought it might be interesting to compare my implementations against the C# built in <code>List&lt;T&gt;.Sort</code>. That was where the rabbit hole started. </p>

<h3 id="slow-quicksort">Slow Quicksort</h3>
<p>After finishing an implementation of quicksort I decided to give it a try on some big input. I wrote something to generate integer lists of arbitrary size with uniformly random elements. I ran some benchmarks, and then ran the same input against <code>List&lt;T&gt;.Sort</code>. My implementation was about <b>ten times</b> slower than the built in sort. I thought I must be doing something wrong. There must be some bug in my code. After all, the description of <code>List&lt;T&gt;.Sort</code> states that (from <a href="http://msdn.microsoft.com/en-us/library/b0zbh7b6%28v=vs.110%29.aspx">here</a>)</p>

<ul>
  <li>If the partition size is fewer than 16 elements, it uses an insertion sort algorithm.</li>
  <li>If the number of partitions exceeds 2 * LogN, where N is the range of the input array, it uses a Heapsort algorithm.</li>
  <li>Otherwise, it uses a Quicksort algorithm.</li>
</ul>

<p>So the built-in sorting should be using Quicksort <em>most</em> of the time. I thought my implementation might just be slow, so I started to make some optimizations. My base implementation can be found here:<br />
<a href="https://github.com/mattnedrich/algorithms/tree/master/sorting/quicksort/csharp">https://github.com/mattnedrich/algorithms/tree/master/sorting/quicksort/csharp</a></p>

<p>First I tried to in-line all of the function calls (e.g., <code>ChoosePivotIndex</code> and <code>Swap</code>). This yielded some performance improvement (about 10%). Next I played around with the pivot selection method. I was using a naive random pivot selection so I tried a median of three approach. This actually didn&rsquo;t do much, though I suspect it is because my input was uniformly random. I tried parallelizing the calls to <code>Partition</code>. This actually slowed things down (probably due to the overhead of thread creation). Lastly, I cheated a bit and switched to insertion sort for small inputs, as the library sort does. This yielded the latest performance gain of around 20%. However, after all of my optimization, I was still around eight times slower than <code>List&lt;T&gt;.Sort</code>.</p>

<p>I was stuck, so I posted <a href="http://stackoverflow.com/questions/21818889/why-is-my-c-sharp-quicksort-implementation-significantly-slower-than-listt-sor">this</a>, and learned that the .NET Framework special cases the sorting of built-in types (ints, string, etc). Just how much of a performance gain is achieved by doing this? I wanted to find out so I setup yet another experiment.</p>

<h3 id="int-vs-intinwrapper">Int vs. IntInWrapper</h3>
<p>I compared the performance of <code>List&lt;T&gt;.Sort</code> when using plain ol&rsquo; <code>int</code>&rsquo;s to the performance when using <code>int</code>&rsquo;s in a wrapper class. For the wrapper class I used:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="directive">public</span> <span class="type">class</span> <span class="class">IntInWrapper</span> : IComparable&lt;IntInWrapper&gt;
{
    <span class="directive">public</span> <span class="type">int</span> Val { get; set; }
    <span class="directive">public</span> IntInWrapper(<span class="type">int</span> val)
    {
        Val = val;
    }

    <span class="directive">public</span> <span class="type">int</span> CompareTo(IntInWrapper other)
    {
        <span class="keyword">if</span> (<span class="local-variable">this</span>.Val &lt; other.Val)
            <span class="keyword">return</span> -<span class="integer">1</span>;
        <span class="keyword">else</span> <span class="keyword">if</span> (<span class="local-variable">this</span>.Val == other.Val)
            <span class="keyword">return</span> <span class="integer">0</span>;
        <span class="keyword">else</span>
            <span class="keyword">return</span> <span class="integer">1</span>;
    }
}
</pre></div>
</div>
 </figure></notextile></div>

<p>I benchmarked sorting Lists of various sizes for the two input classes. Results are shown below. For each List size, sorting was performed ten times (each time on a different randomized list), and the average execution time was kept.</p>

<p><img width="450px" src="http://mattnedrich.github.io/images/c_sharp_sorting/ListT.png" /></p>

<p>Sorting on wrapped <code>int</code>&rsquo;s drastically degrades performance compared to plain  <code>int</code>&rsquo;s. From my understanding, this is largely due to the <code>IComparable&lt;T&gt;.CompareTo</code> function pointer being used for element comparison, rather than a simple <code>&gt;</code> or <code>&lt;</code> (which likely trickles down to a single assembly instruction). Anyway, the key learning from this is that use built-in types when you need sorting performance and be aware of the penalty associated with using custom types! </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hide and Seek with OneNote]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/02/14/fun-onenote-problem/"/>
    <updated>2014-02-14T11:26:01-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/02/14/fun-onenote-problem</id>
    <content type="html"><![CDATA[<p>I often open command windows on Windows via <code>shift + right click</code> on a directory. It&rsquo;s a handy way to open a cmd window right where you want one. Ironically the option directly below <code>Open command window here</code> is <code>Open as Notebook in OneNote</code>. I accidentally clicked the OneNote option and it turned out to be a pretty interesting 10 pixel mistake.</p>

<p><img width="250px" src="http://mattnedrich.github.io/images/onenote_fun/onenote_cmd_window.jpg" /></p>

<p>Some backstory - my original intent was to open a cmd window to the source directory of my blog to build it (I&rsquo;m using Octopress). At first it wasn&rsquo;t clear what clicking the OneNote option actually did. However, after a little poking around I was seeing a <code>OneNote Table Of Content.onetoc2</code> file in every directory of my blog output. That is, I didn&rsquo;t see these in the blog source directory, but every time I would build my blog they would show up in <strong>every</strong> output directory.</p>

<p>I tried grepping for interesting files/directories in the blog source, but nothing showed up. I tried manually browsing through the source directory structure. Also note that I had show hidden files and folders turned on. Still, I found nothing.</p>

<p>After some digging, and a lot of help from this <a href="http://www.tigraine.at/2009/09/01/howto-get-rid-of-onetoc2-files-using-powershell">post</a> it turns out that there were several OneNote files embedded and hidden in every directory of my blog source. The only way I was able to see them was to display them in a cmd window via</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
dir /A:H /B
</pre></div>
</div>
 </figure></notextile></div>

<p>After it was clear that they were indeed there but hidden, I was able to delete them via</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
del /S /A:H *.onetoc2
</pre></div>
</div>
 </figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-Means in Python]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/02/09/k-means-implementation-in-python/"/>
    <updated>2014-02-09T14:45:25-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/02/09/k-means-implementation-in-python</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve written a general K-Means implementation in Python. It can be found here:<br />
<a href="https://github.com/mattnedrich/algorithms/tree/master/clustering/kmeans/python">https://github.com/mattnedrich/algorithms/tree/master/clustering/kmeans/python</a></p>

<p>Everything is contained in one file, <code>kmeans.py</code> The usage model is the following</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">import</span> <span class="include">kmeans</span>

<span class="comment"># observations</span>
<span class="comment">#  - list of items to be clustered</span>
<span class="comment"># numMeans</span>
<span class="comment">#  - number of clusters desired</span>
<span class="comment"># featureVectorFunc</span>
<span class="comment">#  - function that extracts a feature vector (in the form of a Tuple) from an observation</span>
<span class="comment"># maxIterations</span>
<span class="comment">#  - optional param, can be used to set a max number of iterations</span>
[clusters, error, numIter] = kmeans.cluster(observations, numMeans, featureVectorFunc, maxIterations)
</pre></div>
</div>
 </figure></notextile></div>

<p>My goal was to make something simple and accessible that doesn&rsquo;t require additional libraries (e.g., numpy is not required). The user must do one thing - provide a <code><span class="CodeRay">featureVectorFunc</span></code> that takes in an observation from the input, and returns a feature vector in the form of a Tuple. This function would look something like this:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">def</span> <span class="function">someFeatureVectorFunc</span>(obs):
    featureVectorAsTuple = extractFeatureVector(obs)
    <span class="keyword">return</span> featureVectorAsTuple
</pre></div>
</div>
 </figure></notextile></div>

<p>The output clusters is a list of cluster objects. Each cluster has a field called <code><span class="CodeRay">observations</span></code>. This is subset of the original input observations (stored in a list).</p>

<div class="note">
<b>This implementation is not designed for speed</b>. Rather, it&#8217;s designed for flexibility. The user can specify a list containing any type of objects. The required featureVectorFunc is used by the K-Means implementation to extract feature vectors from the input observations.
</div>

<h2 id="example-results">Example Results</h2>
<p>I ran my implementation on the three synthetic datasets below. For each dataset, I used K=5. I also ran K-Means 10 times and selected the result with the lowest error.</p>

<h3 id="dataset-1">Dataset 1</h3>

<p><img width="350px" src="http://mattnedrich.github.io/images/kmeans_python/basicDataPoints.png" />
<img width="350px" src="http://mattnedrich.github.io/images/kmeans_python/basicDataResult.png" /></p>

<h3 id="dataset-2">Dataset 2</h3>

<p><img width="350px" src="http://mattnedrich.github.io/images/kmeans_python/roundDataSetPoints.png" />
<img width="350px" src="http://mattnedrich.github.io/images/kmeans_python/roundDataSetResult.png" /></p>

<h3 id="dataset-3">Dataset 3</h3>

<p><img width="350px" src="http://mattnedrich.github.io/images/kmeans_python/gaussianWithRoundPoints.png" />
<img width="350px" src="http://mattnedrich.github.io/images/kmeans_python/gaussianWithRoundResult.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking C# Code]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/02/07/benchmarking-c-number/"/>
    <updated>2014-02-07T00:01:49-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/02/07/benchmarking-c-number</id>
    <content type="html"><![CDATA[<p>I occasionally need to benchmark some code. In C#, this can easily be done using the Stopwatch class. After writing the same boilerplate Stopwatch wrapper enough times I decided to throw together a simple Benchmarker class.</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="directive">public</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">Benchmarker</span>
{
    <span class="directive">public</span> <span class="directive">static</span> TimeSpan RunOnce(<span class="predefined-type">Action</span> action)
    {
        Stopwatch stopwatch = Stopwatch.StartNew();
        action();
        stopwatch.Stop();
        <span class="keyword">return</span> stopwatch.Elapsed;
    }
    
    <span class="directive">public</span> <span class="directive">static</span> IEnumerable&lt;TimeSpan&gt; RunMany(<span class="predefined-type">Action</span> action, <span class="type">int</span> runCount)
    {
        <span class="predefined-type">List</span>&lt;TimeSpan&gt; results = <span class="keyword">new</span> <span class="predefined-type">List</span>&lt;TimeSpan&gt;();
        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="integer">0</span>; i &lt; runCount; i++)
            results.Add(RunOnce(action));
        <span class="keyword">return</span> results;
    }
}
</pre></div>
</div>
 </figure></notextile></div>

<p>This allows any code to be easily called and timed. For example, suppose you have a void Method called <code>Foo</code>. This can be timed via:</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
TimeSpan timespan = Benchmarker.RunOnce(() =&gt; Foo());
</pre></div>
</div>
 </figure></notextile></div>

<p>Similarly, if you want to benchmark a method that happens to return something like this</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">int</span> bar = FooWithReturnValue();
</pre></div>
</div>
 </figure></notextile></div>

<p>you can use the above Benchmarker class via</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">int</span> bar;
TimeSpan timespan = Benchmarker.RunOnce(() =&gt; bar = FooWithReturnValue());
</pre></div>
</div>
 </figure></notextile></div>

<p>I&rsquo;ve put this on github:
<a href="https://github.com/mattnedrich/tools/tree/master/csharp/Benchmarker.cs">https://github.com/mattnedrich/tools/tree/master/csharp/Benchmarker.cs</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android Virtual Machine Acceleration]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/02/01/android-virtual-machine-acceleration/"/>
    <updated>2014-02-01T14:15:29-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/02/01/android-virtual-machine-acceleration</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve dabbled in Android development in the past, but haven&rsquo;t done much with emulation. In my experience the Android emulators are too slow to use, and I&rsquo;ve usually used my phone for testing/debugging.</p>

<p>Intel&rsquo;s <a href="http://software.intel.com/en-us/android/articles/intel-hardware-accelerated-execution-manager">Hardware Accelerated Execution Manager</a> (HAXM) uses CPU virtualization to speed up Android emulators. I wanted to give it a try. </p>

<h2 id="cpu-compatibility">CPU Compatibility</h2>
<p>I have a pretty old desktop machine, so I wasn&rsquo;t even sure if my CPU supported hardware virtualization. Poking around a bit, there are a few Windows utilities that can be used to determine if virtualizaiton is enabled on your CPU.</p>

<ul>
  <li><a href="http://www.intel.com/support/processors/sb/cs-030729.htm">http://www.intel.com/support/processors/sb/cs-030729.htm</a></li>
  <li><a href="http://www.microsoft.com/en-us/download/details.aspx?id=592">http://www.microsoft.com/en-us/download/details.aspx?id=592</a></li>
  <li><a href="https://www.grc.com/securable.htm">https://www.grc.com/securable.htm</a></li>
</ul>

<p>The Intel utility looks like this:</p>

<p><img width="525px" height="384px" src="http://mattnedrich.github.io/images/android_virtual/vmx_enabled.jpg" /></p>

<h2 id="results">Results</h2>
<p>With virtualization verified and enabled, I followed <a href="http://developer.android.com/tools/devices/emulator.html#acceleration">this</a> writeup to install everything and setup an x86 Android emulator.</p>

<p>So how did it work? It wasn&rsquo;t that much faster, but there was a noticeable difference. The HAXM emulator booted in 182 seconds, compared to 282 seconds for the ARM emulator. With both emulators open, I could click on a button (e.g., open the Settings app) and the HAXM emulator would finish first, usually by 1-5 seconds depending on the task. As a disclaimer, I was using a pretty old computer. I&rsquo;m not sure what impact that had on my experience.</p>

<p>Regardless, while there was some speedup, it&rsquo;s nowhere close to testing/debugging on a real phone (as is to be expected). The emulator is nice for testing different phones/configurations, but in general I find it much nicer to just push my application to a real phone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-Means Clustering]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/01/26/k-means-basics/"/>
    <updated>2014-01-26T18:19:21-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/01/26/k-means-basics</id>
    <content type="html"><![CDATA[<h2 id="what-is-k-means">What is K-Means</h2>
<p>Need to cluster some data? K-Means is one of the most common and simplistic clustering algorithms. It works by partitioning a data set into K  different groups (clusters). In this post, we&rsquo;ll walk through the basics of K-Means, explain how it works, and touch on some of its strengths and weaknesses.</p>

<h2 id="the-input">The Input</h2>
<p>The input to K-Means is a set of data and a parameter K, which defines the number of clusters to partition the data into. Each input observation is a vector in d-dimensional space. The number of dimensions doesn&rsquo;t matter.</p>

<h2 id="the-algorithm">The Algorithm</h2>
<p>The goal of the K-Means algorithm is to find a partitioning of the input data that minimizes within-cluster sum of squares (WCSS). Simply put, the goal is to find a clustering such that the points in each cluster are close together. This objective function (we&rsquo;ll call it $J$) can be expressed as:</p>

<script type="math/tex; mode=display">
\begin{align}
J=\sum_{k=1}^K\sum_{i=1}^N\lVert\mu_k-x_i\rVert ^2
\end{align}
</script>

<p>Here, $K$ is the number of clusters, $N$ is the number of points, $\mu_k$ is the cluster centroid for cluster $k$, and $x_i$ corresponds to input observation $i$. Informally, this function computes the variance of the observations around each cluster centroid (for a given set of K centroids). The goal is the minimize this function. Each iteration of the K-Means algorithm reduces the objective function, moving closer to a minimum.</p>

<p>K-Means uses an <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation Maximization</a> approach to partition the input data and minimize the WCSS. Conceptually, we start with an initial set of cluster centroids (with an implicitly defined WCSS score). The goal is to figure out how to move these centroids around to make the WCSS score as small as possible. The algorithm can be expressed as</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="float">1.</span> Initialize cluster centroids
<span class="keyword">while</span> <span class="keyword">not</span> converged:
    <span class="float">2.</span> Cluster Assignment - assign each observation to the nearest cluster centroid
    <span class="float">3.</span> Update Centroids - update the cluster centroid locations
</pre></div>
</div>
 </figure></notextile></div>

<p>Let&rsquo;s go over each of these steps.</p>

<h4 id="initialize-cluter-centroids"><strong>1. Initialize Cluter Centroids</strong></h4>
<p>The first step is to choose an initial set of K cluster centroid locations. This can be done many ways. One approach is to choose K random locations in whatever dimensional space you&rsquo;re working in. This approach may work, but in practice it may be difficult to choose these locations well. For example, it&rsquo;s undesirable to choose locations very far away from any of the data. Another, perhaps better, approach is to choose K random observations from the input dataset.</p>

<h4 id="cluster-assignment"><strong>2. Cluster Assignment</strong></h4>
<p>In the cluster assignment step each observation in the dataset is assigned to a cluster centroid. This is done by assigning each observation to it&rsquo;s nearest centroid.</p>

<div class="note">
<b>Note:</b> The true objective of this step is to minimize the WCSS objective. It just so happens that assinging each observation to the its nearest cluster achieves this when working in Euclidean space. This may not be the case if other distance metrics are used. 
</div>

<h4 id="cluster-centroid-update"><strong>3. Cluster Centroid Update</strong></h4>
<p>Next, in the cluster centroid update step, each cluster centroid is updated to be the center (e.g., mean) of the observations assigned to it.</p>

<p>Steps <strong>2</strong> and <strong>3</strong> are repeated until the algorithm converges. There are a few commonly used ways to determine convergence.</p>

<ol>
  <li><strong>Fixed Iteration Number</strong> - K-Means can be terminated after running for a fixed number of iterations. Obviously, this approach does not guarantee that the algorithm has converged, but can be useful when an approximate solution is sufficient, or termination is difficult to detect.</li>
  <li><strong>No Cluster Assignment Changes</strong> - This is the most common approach. If no observations change cluster membership between iterations, this indicates that the algorithm will not continue further, and can be terminated.</li>
  <li><strong>Error Threshold</strong> - An error threshold can also be used to terminate the algorithm. However, choosing a good threshold is problem and domain specific (e.g., a threshold that works well for one application may not work well for another).</li>
</ol>

<h2 id="example">Example</h2>
<p>K-Means is often best understood visually, so let&rsquo;s walk through an example. Suppose you have the following data in two dimensional space.</p>

<p><img width="400px" height="300px" src="http://mattnedrich.github.io/images/kmeans_basics/0ab.png" /></p>

<p>The first step is to choose K starting cluster centers. For our example we will set K=5. For our example we&rsquo;ll randomly select five observations from our data set to be the initial cluster centroids:</p>

<p><img width="400px" height="300px" src="http://mattnedrich.github.io/images/kmeans_basics/0abc.png" /></p>

<p>With the initial cluster centroids selected, steps <strong>2</strong> and <strong>3</strong> are iterated as described above.</p>

<p><img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00001_a.png" />
<img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00001_b.png" />
<img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00001_c.png" /></p>

<p><img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00002_a.png" />
<img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00002_b.png" />
<img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00002_c.png" /></p>

<p><img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00003_a.png" />
<img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00003_b.png" />
<img width="270px" src="http://mattnedrich.github.io/images/kmeans_basics/00003_c.png" /></p>

<h2 id="other-comments">Other Comments</h2>

<h3 id="repeated-runs">Repeated Runs</h3>
<p>It&rsquo;s very important to note that K-Means is notorious for getting stuck in local minima. This means that there is no guarantee that running K-Means will actually find the optimal solution for a given starting point (set of initial cluster centroids). To overcome this, K-Means is often run multiple times with different initial cluster centroids, and the clustering with the lowest error (WCSS) is chosen.</p>

<h3 id="choosing-k">Choosing K</h3>
<p>This <a href="http://www.youtube.com/watch?v=Lz0sXs12uPk">video</a> from Andrew Ng&rsquo;s Machine Learning course on <a href="http://www.coursera.org">Coursera</a> does a great job summarizing the task of choosing K. In general, there is no great general purpose method. Choosing K often requires manual input based on data visualizaiton or other domain knowledge. Interestingly, for some applications, K may already be known. The T-Shirt sizing example in the video is a great example of this. For these types of problems the goal is to partition the data into a pre-defined number of sets, rather than learning how many distinct sub-sets make up the original input data.</p>

<h3 id="strengths-and-weaknesses">Strengths and Weaknesses</h3>
<p><strong>Strengths:</strong> K-Means is great because it&rsquo;s simple and fast. There are several off the shelf implementations available on the internet, and if they&rsquo;re not good enough, it&rsquo;s not terribly difficult to implement K-Means on your own.
<br />
<strong>Weaknesses</strong>:The main weakness of K-Means is that it doesn&rsquo;t work that well for non-spherical clusters. Also, as we discussed earlier, it&rsquo;s also notorius for getting stuck in local minima, and requires the user to choose K.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LaTeX Math in Octopress]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/01/18/latex-math-in-octopress/"/>
    <updated>2014-01-18T17:16:43-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/01/18/latex-math-in-octopress</id>
    <content type="html"><![CDATA[<p>I found this post helpful when trying to enable LaTeX style math formulas in Octopress:</p>

<p><a href="http://blog.zhengdong.me/2012/12/19/latex-math-in-octopress">http://blog.zhengdong.me/2012/12/19/latex-math-in-octopress</a></p>

<p>It&rsquo;s also worth noting that when you change Octopress themes <code>source/_includes/custom/head.html</code> seems to get overwritten, so you have to re-add the provided snippet.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Finding Files on Windows]]></title>
    <link href="http://mattnedrich.github.io/blog/2014/01/18/finding-files-on-windows/"/>
    <updated>2014-01-18T16:53:50-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2014/01/18/finding-files-on-windows</id>
    <content type="html"><![CDATA[<p>I often find myself in situations where I know a file exists but can&rsquo;t find it.</p>

<p>On Windows, the following can be used to search for a file from the current working directory</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
dir /s /b | findstr [search string]
</pre></div>
</div>
 </figure></notextile></div>

<p>Here <code>/s</code> denotes a recursive search, and <code>/b</code> allows the full paths of the files to be printed (omit that if the full paths are not desired). The results from the <code>dir</code> are piped to a <code>findstr</code> command where a filter can be applied to find the desired file(s).</p>

<p>If you have <code>grep</code> installed (e.g., via the <a href="http://sourceforge.net/projects/unxutils/">unix utilities</a>) you can replace <code>findstr</code> with <code>grep</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
dir /s /b | grep [search string]
</pre></div>
</div>
 </figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Good ol' C Casting]]></title>
    <link href="http://mattnedrich.github.io/blog/2013/03/25/good-ol-c-casting/"/>
    <updated>2013-03-25T21:28:00-04:00</updated>
    <id>http://mattnedrich.github.io/blog/2013/03/25/good-ol-c-casting</id>
    <content type="html"><![CDATA[<p>Understanding data representation is important. Consider the following C code.</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="predefined-type">float</span> foo = <span class="integer">5</span>;
<span class="predefined-type">int</span> bar = foo;
printf(<span class="string"><span class="delimiter">&quot;</span><span class="content">%d</span><span class="delimiter">&quot;</span></span>, bar);
</pre></div>
</div>
 </figure></notextile></div>

<p>This little C snippet outputs the number 5, as is expected. Now, let&rsquo;s make a slight change. Instead of assigning foo to bar, lets cast it to an int by casting the address that foo is stored to an int pointer and then dereferencing it.</p>

<div class="bogus-wrapper"><notextile><figure class="code"> <div class="CodeRay">
  <div class="code"><pre>
<span class="predefined-type">float</span> foo = <span class="integer">5</span>;
<span class="predefined-type">int</span> bar = *(<span class="predefined-type">int</span>*)&amp;foo;
printf(<span class="string"><span class="delimiter">&quot;</span><span class="content">%d</span><span class="delimiter">&quot;</span></span>, bar);
</pre></div>
</div>
 </figure></notextile></div>

<p>This time around we get the value 1084227584. That&rsquo;s certainly not 5 (as we observed the first time around), so what&rsquo;s going on here. As you may already know, integer and floating point numbers are stored differently.</p>

<h3 id="floating-point-numbers">Floating Point Numbers</h3>
<p>Let&rsquo;s start with how the float value is stored in memory. Wikipedia gives us this handy IEE 754 single precision binary floating point representation diagram:</p>

<p><img width="500px" src="http://mattnedrich.github.io/images/c_casting/500px-Float_example.svg.png" /></p>

<p>The most significant bit denotes the sign of the number. The next 8 bits store what&rsquo;s called the exponent. Finally the last 23 bits store the fraction, also known as the mantissa. When determining the value stored in such a floating point number, the following approach is used.</p>

<script type="math/tex; mode=display">(1.0+fraction)\cdot 2^{(exponent-127)}</script>

<p>In our example we were storing the value <script type="math/tex">5</script>. This can be computed as <script type="math/tex">1.25\cdot 2^2</script>. As a 32-bit floating point number this is stored as </p>

<p><script type="math/tex">{01000000101000000000000000000000}_2</script>. </p>

<p>We can partition this binary number into the three different segments defined in the above diagram:</p>

<p><script type="math/tex">\color{blue}0\color{green}{10000001}\color{red}{01000000000000000000000}_2</script>. </p>

<p>Here, the sign bit <script type="math/tex">\color{blue}{0}_2</script> tells us the number is positive. The exponent <script type="math/tex">\color{green}{100000001}_2</script> is equal to <script type="math/tex">129-127=2</script> in base ten. Finally the fraction <script type="math/tex">\color{red}{01000000000000000000000}_2</script> gives us <script type="math/tex">2^{-2}</script>, also known as <script type="math/tex">0.25</script>. If we put these three pieces together we see that they give us <script type="math/tex">1.25\cdot 2^2 = 5</script>.</p>

<h3 id="integer-numbers">Integer Numbers</h3>
<p>Integers are stored in standard <a href="http://en.wikipedia.org/wiki/Two%27s_complement">two&rsquo;s complement</a> format. The number five, would be stored as <script type="math/tex">{101}_2</script>. In two&rsquo;s complement, this is <script type="math/tex">1\cdot 2^2 + 0\cdot 2^1 + 1\cdot 2^0</script>. We can easily see that this two&rsquo;s complement representation of <script type="math/tex">5</script> is very different than the above floating point representation.</p>

<h3 id="conclusion">Conclusion</h3>
<p>In the first snippet when we assigned foo to bar an implicit floating point to two&rsquo;s complement conversion happened. The result of this conversion was then stored in bar&rsquo;s memory location. In the second snippet we cast foo to an integer and the bits that make up foo were copied verbatim and stored as bar. Because no conversion occurred we ended up with a different result because we treated the bits from the floating point representation of 5 as if they were two&rsquo;s complement.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Points, Lines, and Calculus]]></title>
    <link href="http://mattnedrich.github.io/blog/2013/03/03/points-lines-and-calculus/"/>
    <updated>2013-03-03T18:49:00-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2013/03/03/points-lines-and-calculus</id>
    <content type="html"><![CDATA[<p>Suppose we want to model some data using a line. This type of problem is very common and can be found in a variety of fields. It&rsquo;s also a great example of how calculus can be applied to solve problems. Let&rsquo;s say that our data set looks something like this:</p>

<p><img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/points.jpg" /></p>

<p>The goal is to find a line that fits the data well - probably something like this:</p>

<p><img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/points_plot_example_solution.jpg" /></p>

<p>To find the red line we&rsquo;re going to use the following approach. Let&rsquo;s consider all two dimensional lines. We&rsquo;re going to call this set $L$. We want to find some line $l$ in $L$ that models our data well. Seems obvious, right? The big question is how to we find $l$. To do this we need is a mechanism for scoring each line $l$. That way we could theoretically score every line and select the best one.</p>

<p>Scoring every line is impossible, as there are infinite lines. We can, however, develop a mechanism for scoring a single line. Recall the standard line equation $y = mx + b$. Here, $m$ is the slope of the line and $b$ is the y-intercept. With this equation we can define any two dimensional line. Now suppose we have a line defined by some $(m,b)$ pair. We&rsquo;re going to use the following approach to compute the error <script type="math/tex">e_{(m,b)} </script> for line <script type="math/tex">(m,b)</script>.</p>

<script type="math/tex; mode=display">e_{(m,b)}=\sum_{p\in P}\left( \color{red}{p_y}-( \color{blue}{mp_x+b})\right)^2</script>

<p>Here <script type="math/tex">P</script> is the original set of points. For each point <script type="math/tex">p=(p_x,p_y)</script> from <script type="math/tex">P</script> we use the slope and y-intercept from the candidate line to compute a prediction for <script type="math/tex">p_y</script> based on <script type="math/tex">m</script> and <script type="math/tex">b</script>. This estimate is the blue portion of the above equation. The error for this point <script type="math/tex">p</script> is computed by comparing this blue value to the red value (via subtraction, and then squaring it). This process is repeated for every point, and the errors are added up to produce the total error <script type="math/tex">e_{(m,b)}</script> for the candidate line <script type="math/tex">(m,b)</script>.</p>

<div class="note">
<b>Note:</b> If the candidate line defined by $m$ and $b$ does a good job at fitting the point $p$ then $$p_y - (mp_x+b)$$ will be small. If the line does a good job for all points $p\in P$ then the sum of these errors $e_{(m,b)}$ will be small.
</div>
<p><br /></p>

<p>Now, for any set of candidate lines (i.e., $(m,b)$ pairs) we can compute error values for all of them and select the one with the smallest error. Although we&rsquo;ve made progress there are still many important questions to answer. For example - how are we going to be sure that find the best line? We could randomly choose some lines, but the set of lines to consider is infinite. We need a structured method to search for this magical &ldquo;lowest error&rdquo; line.</p>

<p>This is where things get interesting. The fact that we can define a line as any $(m,b)$ pair is important. By doing so we can define a <em>line space</em> that looks something like this:</p>

<p><img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/mb_space.jpg" /></p>

<p>This space is similar to the standard 2D $(x,y)$ space that most people are familiar with, but instead of $x$ and $y$ we have $m$ and $b$. Every location in this space defines a unique $(m,b)$ pair, and thus a unique line. Some where in this space exists the magical line that produces the lowest amount of error for our data set. The above error function $e_{(m,b)}$ works over this $(m,b)$ space. Using our input data set (which is fixed) if we computed the error for every line in our $(m,b)$ space we could visualize this error function as a surface where the height of the surface corresponds to the error at that location in the $(m,b)$ space (the error that the line yielded given our data set). We might imagine such a surface to look something like this:</p>

<p><img width="350px" src="http://mattnedrich.github.io/images/points_lines_calculus/expected_error_surface.jpg" /></p>

<p>Although this isn&rsquo;t what our surface looks like for our data set, it gives us some intuition as to what we might expect. Somewhere in this surface exists a minimum (e.g., the lowest location on the surface). This location corresponds to a $(m,b)$ pair - the line that we&rsquo;re looking for that produces the lowest amount of error for our data set. Our problem is basically reduced to finding this minimum. We can do this using two approaches - both involve calculus.</p>

<h3 id="method-1---closed-form-solution"><strong>Method 1</strong> - Closed Form Solution</h3>
<p>Recall our error function </p>

<script type="math/tex; mode=display">e_{(m,b)}=\sum_{p\in P}\left(p_y-(mp_x+b)\right)^2</script>

<p>We are interested in finding the line ($m,b$ pair) that minimizes this function given our data set. If you&rsquo;re familiar with calculus you should realize that in order to do this we can take the derivative of our error function, set it equal to zero, and solve for $m$ and $b$. The derivative evaluated at zero will give use the line ($m,b$ pair) that yields the minimum error for our data set. Our error function depends on two variables - $m$ and $b$. Therefore we will need to take two partial derivatives, one with respect to $m$ and one with respect to $b$. These partial derivatives can be computed as:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial m} = 2 \sum_{i=1}^{N}-x_i(y_i-(m x_i+b))</script>

<script type="math/tex; mode=display">\frac{\partial}{\partial b} = 2 \sum_{i=1}^{N}-(y_i-(m x_i+b))</script>

<p>The problem now reduces to setting each of the partial derivatives equal to zero and solving for $m$ and $b$. We have two equations and two unknowns, so this is certainly a solvable problem. If you&rsquo;re interested the closed form solution works out to be:</p>

<script type="math/tex; mode=display">m = \frac{N\sum_{i=1}^Nx_iy_i-\sum_{i=1}^Nx_i\sum_{i=1}^Ny_i}{N\sum_{i=1}^Nx_i^2-(\sum_{i=1}^Nx_i)^2}</script>

<script type="math/tex; mode=display">b = \frac{\sum_{i=1}^Ny_i}{N}-m\frac{\sum_{i=1}^Nx_i}{N}</script>

<div class="note">
<b>Note:</b> Here $N$ denotes the number of points in the input data set. There are also several terms that have semantic meanings - for example $\frac{\sum_{i=1}^Ny_i}{N}$ and $\frac{\sum_{i=1}^Nx_i}{N}$ correspond to the average y and x values (respectively) for the points in our data set.
</div>
<p><br /></p>

<p>Using this solution we end up with the following line for out data set:</p>

<p><img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/line_solution.jpg" /></p>

<p>The closed form method is simple, but it requires some algebra to solve for $m$ and $b$. For other problems this can be extremely expensive. The next method eliminates the need to solve for $m$ and $b$ directly. Rather, it utilizes a nice property of the partial deriviaties we took.</p>

<h3 id="method-2---gradient-search"><strong>Method 2</strong> - Gradient Search</h3>
<p>The two partial derivatives above are also known as our error function&rsquo;s gradient. The gradient is extremely useful because it can help us find $m$ and $b$ without actually solving the system of equations. It does this by acting as a compass and allowing us to search around the $(m,b)$ space, always pointing us in the direction of decreasing slope for our error function. This type of search is called gradient descent. The gradient evaluated at any given $(m,b)$ point of the error function gives us the slope of the function at that location (e.g., the direction in $(m,b)$ space to move to be at a lower location on the error surface. Since we&rsquo;re interested in the lowest location of our error function (the minimum) we can use the slope to move toward the solution. This is an iterative process where we evaluate the gradient, move toward the minimum, re-evaluate the gradient at the new location, move again, and continue this process until we reach a location where the slope is zero (e.g., the minimum).</p>

<div class="note">
<b>Note:</b> The gradient is evaluated by plugging a $m$ and $b$ value into the partial derivative equations. The result, $\frac{\partial}{\partial m}$ and $\frac{\partial}{\partial b}$ give us a vector that points in the direction of decreasing slope in the $(m,b)$ space. How far we move in that direction is a parameter that can be varied known as the step size. Choosing a good step size is important. A step size that is too large may never converge well, as it might step over the minimum location each time. A step size that is too small may take a long time to find a solution as you will be moving around very slowly.
</div>
<p><br /></p>

<p>The actual error function for our problem given our input data looks like this:</p>

<div>
<img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/actual_error_surface.jpg" />
<img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/contour_error_surface.jpg" />
</div>
<p><br />
Although it may be difficult to tell, there is an actual minimum location on this surface. It&rsquo;s easier to see using the contour plot on the right. In fact, it&rsquo;s even easier to observe if you look at the log of the error function. It&rsquo;s often useful to view the log of a function because log scales compress wide ranging data into a smaller window. Here is the log plot of our error function, and the corresponding contour plot:</p>

<div>
<img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/actual_log_error_surface.jpg" />
<td><img width="325px" src="http://mattnedrich.github.io/images/points_lines_calculus/contour_log_error_surface.jpg" /></td>
</div>
<p><br />
From this, it can be seen that there is a strong minimum location in our error function.</p>

<p>Gradient search can be very useful. For our problem it isn&rsquo;t too difficult to solve for $m$ and $b$ directly. However, for many problems it is often difficult or very expensive to compute a closed form solution. In such cases, the gradient can usually be computed or at least estimated and is used to search for a solution. Our problem was also very easy in that our error function was monotonically decreasing. That is, it had one minimum location and all other locations sloped toward that minimum. More complex functions can have several local minima or maxima. This creates a mathematical terrain that is difficult to navigate as it is easy to get suck in local minima/maxima (e.g. other locations where the slope is equal to zero).</p>

]]></content>
  </entry>
  
</feed>
