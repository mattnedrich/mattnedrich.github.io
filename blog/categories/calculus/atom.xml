<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Calculus | Matt Nedrich]]></title>
  <link href="http://mattnedrich.github.io/blog/categories/calculus/atom.xml" rel="self"/>
  <link href="http://mattnedrich.github.io/"/>
  <updated>2014-02-20T10:01:28-05:00</updated>
  <id>http://mattnedrich.github.io/</id>
  <author>
    <name><![CDATA[Matt Nedrich]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Points, Lines, and Calculus]]></title>
    <link href="http://mattnedrich.github.io/blog/2013/03/03/points-lines-and-calculus/"/>
    <updated>2013-03-03T18:49:00-05:00</updated>
    <id>http://mattnedrich.github.io/blog/2013/03/03/points-lines-and-calculus</id>
    <content type="html"><![CDATA[<p>Suppose we want to model some data using a line. This type of problem is very common and can be found in a variety of fields. It&rsquo;s also a great example of how calculus can be applied to solve problems. Let&rsquo;s say that our data set looks something like this:</p>

<p><img width="325px" src="/images/points_lines_calculus/points.jpg" /></p>

<p>The goal is to find a line that fits the data well - probably something like this:</p>

<p><img width="325px" src="/images/points_lines_calculus/points_plot_example_solution.jpg" /></p>

<p>To find the red line we&rsquo;re going to use the following approach. Let&rsquo;s consider all two dimensional lines. We&rsquo;re going to call this set $L$. We want to find some line $l$ in $L$ that models our data well. Seems obvious, right? The big question is how to we find $l$. To do this we need is a mechanism for scoring each line $l$. That way we could theoretically score every line and select the best one.</p>

<p>Scoring every line is impossible, as there are infinite lines. We can, however, develop a mechanism for scoring a single line. Recall the standard line equation $y = mx + b$. Here, $m$ is the slope of the line and $b$ is the y-intercept. With this equation we can define any two dimensional line. Now suppose we have a line defined by some $(m,b)$ pair. We&rsquo;re going to use the following approach to compute the error <script type="math/tex">e_{(m,b)} </script> for line <script type="math/tex">(m,b)</script>.</p>

<script type="math/tex; mode=display">e_{(m,b)}=\sum_{p\in P}\left( \color{red}{p_y}-( \color{blue}{mp_x+b})\right)^2</script>

<p>Here <script type="math/tex">P</script> is the original set of points. For each point <script type="math/tex">p=(p_x,p_y)</script> from <script type="math/tex">P</script> we use the slope and y-intercept from the candidate line to compute a prediction for <script type="math/tex">p_y</script> based on <script type="math/tex">m</script> and <script type="math/tex">b</script>. This estimate is the blue portion of the above equation. The error for this point <script type="math/tex">p</script> is computed by comparing this blue value to the red value (via subtraction, and then squaring it). This process is repeated for every point, and the errors are added up to produce the total error <script type="math/tex">e_{(m,b)}</script> for the candidate line <script type="math/tex">(m,b)</script>.</p>

<div class="note">
<b>Note:</b> If the candidate line defined by $m$ and $b$ does a good job at fitting the point $p$ then $$p_y - (mp_x+b)$$ will be small. If the line does a good job for all points $p\in P$ then the sum of these errors $e_{(m,b)}$ will be small.
</div>
<p><br /></p>

<p>Now, for any set of candidate lines (i.e., $(m,b)$ pairs) we can compute error values for all of them and select the one with the smallest error. Although we&rsquo;ve made progress there are still many important questions to answer. For example - how are we going to be sure that find the best line? We could randomly choose some lines, but the set of lines to consider is infinite. We need a structured method to search for this magical &ldquo;lowest error&rdquo; line.</p>

<p>This is where things get interesting. The fact that we can define a line as any $(m,b)$ pair is important. By doing so we can define a <em>line space</em> that looks something like this:</p>

<p><img width="325px" src="/images/points_lines_calculus/mb_space.jpg" /></p>

<p>This space is similar to the standard 2D $(x,y)$ space that most people are familiar with, but instead of $x$ and $y$ we have $m$ and $b$. Every location in this space defines a unique $(m,b)$ pair, and thus a unique line. Some where in this space exists the magical line that produces the lowest amount of error for our data set. The above error function $e_{(m,b)}$ works over this $(m,b)$ space. Using our input data set (which is fixed) if we computed the error for every line in our $(m,b)$ space we could visualize this error function as a surface where the height of the surface corresponds to the error at that location in the $(m,b)$ space (the error that the line yielded given our data set). We might imagine such a surface to look something like this:</p>

<p><img width="350px" src="/images/points_lines_calculus/expected_error_surface.jpg" /></p>

<p>Although this isn&rsquo;t what our surface looks like for our data set, it gives us some intuition as to what we might expect. Somewhere in this surface exists a minimum (e.g., the lowest location on the surface). This location corresponds to a $(m,b)$ pair - the line that we&rsquo;re looking for that produces the lowest amount of error for our data set. Our problem is basically reduced to finding this minimum. We can do this using two approaches - both involve calculus.</p>

<h3 id="method-1---closed-form-solution"><strong>Method 1</strong> - Closed Form Solution</h3>
<p>Recall our error function </p>

<script type="math/tex; mode=display">e_{(m,b)}=\sum_{p\in P}\left(p_y-(mp_x+b)\right)^2</script>

<p>We are interested in finding the line ($m,b$ pair) that minimizes this function given our data set. If you&rsquo;re familiar with calculus you should realize that in order to do this we can take the derivative of our error function, set it equal to zero, and solve for $m$ and $b$. The derivative evaluated at zero will give use the line ($m,b$ pair) that yields the minimum error for our data set. Our error function depends on two variables - $m$ and $b$. Therefore we will need to take two partial derivatives, one with respect to $m$ and one with respect to $b$. These partial derivatives can be computed as:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial m} = 2 \sum_{i=1}^{N}-x_i(y_i-(m x_i+b))</script>

<script type="math/tex; mode=display">\frac{\partial}{\partial b} = 2 \sum_{i=1}^{N}-(y_i-(m x_i+b))</script>

<p>The problem now reduces to setting each of the partial derivatives equal to zero and solving for $m$ and $b$. We have two equations and two unknowns, so this is certainly a solvable problem. If you&rsquo;re interested the closed form solution works out to be:</p>

<script type="math/tex; mode=display">m = \frac{N\sum_{i=1}^Nx_iy_i-\sum_{i=1}^Nx_i\sum_{i=1}^Ny_i}{N\sum_{i=1}^Nx_i^2-(\sum_{i=1}^Nx_i)^2}</script>

<script type="math/tex; mode=display">b = \frac{\sum_{i=1}^Ny_i}{N}-m\frac{\sum_{i=1}^Nx_i}{N}</script>

<div class="note">
<b>Note:</b> Here $N$ denotes the number of points in the input data set. There are also several terms that have semantic meanings - for example $\frac{\sum_{i=1}^Ny_i}{N}$ and $\frac{\sum_{i=1}^Nx_i}{N}$ correspond to the average y and x values (respectively) for the points in our data set.
</div>
<p><br /></p>

<p>Using this solution we end up with the following line for out data set:</p>

<p><img width="325px" src="/images/points_lines_calculus/line_solution.jpg" /></p>

<p>The closed form method is simple, but it requires some algebra to solve for $m$ and $b$. For other problems this can be extremely expensive. The next method eliminates the need to solve for $m$ and $b$ directly. Rather, it utilizes a nice property of the partial deriviaties we took.</p>

<h3 id="method-2---gradient-search"><strong>Method 2</strong> - Gradient Search</h3>
<p>The two partial derivatives above are also known as our error function&rsquo;s gradient. The gradient is extremely useful because it can help us find $m$ and $b$ without actually solving the system of equations. It does this by acting as a compass and allowing us to search around the $(m,b)$ space, always pointing us in the direction of decreasing slope for our error function. This type of search is called gradient descent. The gradient evaluated at any given $(m,b)$ point of the error function gives us the slope of the function at that location (e.g., the direction in $(m,b)$ space to move to be at a lower location on the error surface. Since we&rsquo;re interested in the lowest location of our error function (the minimum) we can use the slope to move toward the solution. This is an iterative process where we evaluate the gradient, move toward the minimum, re-evaluate the gradient at the new location, move again, and continue this process until we reach a location where the slope is zero (e.g., the minimum).</p>

<div class="note">
<b>Note:</b> The gradient is evaluated by plugging a $m$ and $b$ value into the partial derivative equations. The result, $\frac{\partial}{\partial m}$ and $\frac{\partial}{\partial b}$ give us a vector that points in the direction of decreasing slope in the $(m,b)$ space. How far we move in that direction is a parameter that can be varied known as the step size. Choosing a good step size is important. A step size that is too large may never converge well, as it might step over the minimum location each time. A step size that is too small may take a long time to find a solution as you will be moving around very slowly.
</div>
<p><br /></p>

<p>The actual error function for our problem given our input data looks like this:</p>

<div>
<img width="325px" src="/images/points_lines_calculus/actual_error_surface.jpg" />
<img width="325px" src="/images/points_lines_calculus/contour_error_surface.jpg" />
</div>
<p><br />
Although it may be difficult to tell, there is an actual minimum location on this surface. It&rsquo;s easier to see using the contour plot on the right. In fact, it&rsquo;s even easier to observe if you look at the log of the error function. It&rsquo;s often useful to view the log of a function because log scales compress wide ranging data into a smaller window. Here is the log plot of our error function, and the corresponding contour plot:</p>

<div>
<img width="325px" src="/images/points_lines_calculus/actual_log_error_surface.jpg" />
<td><img width="325px" src="/images/points_lines_calculus/contour_log_error_surface.jpg" /></td>
</div>
<p><br />
From this, it can be seen that there is a strong minimum location in our error function.</p>

<p>Gradient search can be very useful. For our problem it isn&rsquo;t too difficult to solve for $m$ and $b$ directly. However, for many problems it is often difficult or very expensive to compute a closed form solution. In such cases, the gradient can usually be computed or at least estimated and is used to search for a solution. Our problem was also very easy in that our error function was monotonically decreasing. That is, it had one minimum location and all other locations sloped toward that minimum. More complex functions can have several local minima or maxima. This creates a mathematical terrain that is difficult to navigate as it is easy to get suck in local minima/maxima (e.g. other locations where the slope is equal to zero).</p>

]]></content>
  </entry>
  
</feed>
